{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c7043a",
   "metadata": {},
   "source": [
    "# Fill That Cart!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1ae0b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Instacart is a grocery delivery platform where customers can place orders and have them delivered, similar to Uber Eats or DoorDash. The dataset provided here is a modified version of the original. Its size was reduced to speed up computations, and missing values and duplicates were intentionally introduced. Care was taken to preserve the original data distributions when making these changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe0f4c",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "The dataset contains five tables. <br>\n",
    "Below is a data dictionary listing each table’s columns and describing the data they contain.\n",
    "instacart_orders.csv: Each row corresponds to an order placed through the Instacart app.\n",
    "\n",
    "- `instacart_orders.csv`\n",
    "    - `'order_id'`: unique ID number identifying each order.\n",
    "    - `'user_id'`: unique ID number identifying each customer account.\n",
    "    - `'order_number'`: the number of times this customer has placed an order.\n",
    "    - `'order_dow'`: day of the week the order was placed (0 = Sunday).\n",
    "    - `'order_hour_of_day'`: hour of the day the order was placed.\n",
    "    - `'days_since_prior_order'`: number of days since this customer’s previous order.\n",
    "- `products.csv`\n",
    "    - `'product_id'`: unique ID number identifying each product.\n",
    "    - `'product_name'`: name of the product.\n",
    "    - `'aisle_id'`: unique ID number identifying each grocery aisle category.\n",
    "    - `'department_id'`: unique ID number identifying each grocery department.\n",
    "- `order_products.csv` \n",
    "    - `'order_id'`: unique ID number identifying each order.\n",
    "    - `'product_id'`: unique ID number identifying each product.\n",
    "    - `'add_to_cart_order'`: sequential order in which each item was added to the cart.\n",
    "    - `'reordered'`: 0 if the customer has never ordered this product before, 1 if they have.\n",
    "- `aisles.csv`\n",
    "    - `'aisle_id'`: unique ID number identifying each grocery aisle category.\n",
    "    - `'aisle'`: name of the aisle.\n",
    "- `departments.csv`\n",
    "    - `'department_id'`: unique ID number identifying each grocery department.\n",
    "    - `'department'`: name of the department.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0474f8da",
   "metadata": {},
   "source": [
    "# Step 1. Data Description\n",
    "Read the data files (instacart_orders.csv, products.csv, aisles.csv, departments.csv, and order_products.csv) using pd.read_csv() with the appropriate parameters to ensure correct data loading. Verify the information for each created DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3f3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31487121",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'instacart_orders.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extract the info from .csv files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_instacart_orders = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstacart_orders.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df_products = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mproducts.csv\u001b[39m\u001b[33m'\u001b[39m, sep= \u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m df_aisles = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33maisles.csv\u001b[39m\u001b[33m'\u001b[39m, sep= \u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'instacart_orders.csv'"
     ]
    }
   ],
   "source": [
    "# Extract the info from .csv files\n",
    "df_instacart_orders = pd.read_csv('instacart_orders.csv', sep= ';')\n",
    "df_products = pd.read_csv('products.csv', sep= ';')\n",
    "df_aisles = pd.read_csv('aisles.csv', sep= ';')\n",
    "df_departments = pd.read_csv('departments.csv', sep= ';')\n",
    "df_order_products = pd.read_csv('order_products.csv', sep= ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af571682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the info contained in file \"orders\", and show the null data sum\n",
    "df_instacart_orders.info()\n",
    "print()\n",
    "df_instacart_orders.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the info contained in file \"products\", and show the null data sum\n",
    "df_products.info()\n",
    "print()\n",
    "df_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the info contained in file \"aisles\", and show the null data sum\n",
    "df_aisles.info()\n",
    "print()\n",
    "df_aisles.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the info contained in file \"departments\", and show the null data sum\n",
    "df_departments.info()\n",
    "print()\n",
    "df_departments.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the info contained in file \"order_products\", and show the null data sum\n",
    "df_order_products.info(show_counts=True)\n",
    "print()\n",
    "df_order_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c4f53",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "At first glance (before displaying the information with .info()), everything seemed fine. However, after examining the first dataset (df_instacart_orders), I noticed that the .csv files were separated by semicolons (;) instead of commas, as is usually the case with this format. <br>\n",
    "After checking the other datasets, I confirmed that the same issue was present in all of them. I then proceeded to import the datasets using the semicolon (;) as the separator, and once the information was displayed, the columns appeared correctly.\n",
    "\n",
    "Analyzing each dataset:\n",
    "- df_instacart_orders: The data in the columns are of type int64, except for the column days_since_prior_order, which contains null values.\n",
    "- df_products: The column product_name is the only one with null values and has a data type of object; the remaining columns are of type int64\n",
    "- df_aisles: All columns contain complete data. Data types are object and int64.\n",
    "- df_departments: All columns contain complete data. Data types are object and int64.\n",
    "- df_order_products: The column add_to_cart_order contains null values and is of type float64. The remaining columns have complete data and are of type int64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a79bd",
   "metadata": {},
   "source": [
    "# Step 2. Data Preprocessing\n",
    "\n",
    "The data preprocessing was carried out as follows:\n",
    "- Verification and correction of data types.\n",
    "- Identification of missing values.\n",
    "- Identification and removal of duplicate values.\n",
    "- Detection and elimination of duplicates (including an explanation of the decision-making process).\n",
    "\n",
    "Look for duplicate values, and validate if need to be removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11606ab9",
   "metadata": {},
   "source": [
    "### orders data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094e8dfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_instacart_orders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Reviewing if there's duplicate values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m duplicados_totales = \u001b[43mdf_instacart_orders\u001b[49m.duplicated().sum()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe duplicate rows are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicados_totales\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_instacart_orders' is not defined"
     ]
    }
   ],
   "source": [
    "# Reviewing if there's duplicate values\n",
    "duplicados_totales = df_instacart_orders.duplicated().sum()\n",
    "print(f\"the duplicate rows are: {duplicados_totales}\")\n",
    "print()\n",
    "\n",
    "# Now, reviewing duplicated rows \n",
    "print(df_instacart_orders[df_instacart_orders.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e488ddf",
   "metadata": {},
   "source": [
    "Do you have duplicate rows? If so, what do they have in common? <br>\n",
    "<br>\n",
    "Yes, there are duplicate rows — a total of 15. <br>\n",
    "After printing the rows, I noticed a common pattern in two columns: <br>\n",
    "\n",
    "- order_dow: 3, where 0 (according to the description) represents Sunday. Therefore, the repeated value across all duplicate rows corresponds to Wednesday.\n",
    "- order_hour_of_day: 2, following the 24-hour format starting at 0 (midnight), which means 2 a.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5806f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on your findings, verify orders made on Wednesday at 2:00am\n",
    "pedidos_miercoles_2am = df_instacart_orders[\n",
    "    (df_instacart_orders['order_dow'] == 3) &\n",
    "    (df_instacart_orders['order_hour_of_day'] == 2)\n",
    "]\n",
    "\n",
    "# print(pedidos_miercoles_2am.head(10))\n",
    "# print(pedidos_miercoles_2am.tail(10))\n",
    "print(pedidos_miercoles_2am.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef33256",
   "metadata": {},
   "source": [
    "What does this result suggest? <br>\n",
    "It suggests that the duplicated rows are indeed redundant data that do not add value to the table. Therefore, it would be worthwhile to remove them in order to maintain a more accurate and reliable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate duplicate values\n",
    "print('number of duplicated rows at the beginning: ', df_instacart_orders.duplicated().sum())\n",
    "df_instacart_orders = df_instacart_orders.drop_duplicates()\n",
    "df_instacart_orders.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db11ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if there's duplicated rows\n",
    "print('number of duplicated rows at the end: ', df_instacart_orders.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16566cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, verify if there's only, duplicated order ID's\n",
    "print('number of duplicate values in column \"order_id\": ', df_instacart_orders['order_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db0992",
   "metadata": {},
   "source": [
    "Findings: <br>\n",
    "<br>\n",
    "According to the dataset (instacart_orders.csv) and the column descriptions, the “order_id” should represent a unique order number. Therefore, any duplicate values in this column indicate incorrect or invalid data. <br>\n",
    "<br>\n",
    "First, I used the .duplicates.sum() method and identified 15 duplicate rows, noticing a pattern in two columns: “order_dow” = 3, which corresponds to Wednesday, and “order_hour_of_day” = 2, which corresponds to 2 a.m. <br>\n",
    "I then applied the drop_duplicates method and reassigned the result to the DataFrame, successfully removing the duplicate rows that added no value to the table. If left undetected, these could have led to inaccurate calculations in later analyses. <br>\n",
    "Finally, I printed the “order_id” column and the cleaned dataset to verify that all duplicate values had been effectively removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49f6f4",
   "metadata": {},
   "source": [
    "### Products data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88122e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if there's duplicated rows\n",
    "print('the number of duplicated rows is: ', df_products.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86530ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's only duplicated product ID's\n",
    "print('the number of duplicated values in \"product_id\" is: ', df_products['product_id'].duplicated().sum())\n",
    "\n",
    "# print(df_products['product_id'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's duplicated name orders (changing name to uppercase to compare)\n",
    "df_products['product_name_uppercase'] = df_products['product_name'].str.upper()\n",
    "duplicate_names = df_products['product_name_uppercase'].duplicated().sum()\n",
    "\n",
    "# null_names = df_products['product_name'].isnull().sum()\n",
    "# print('la cantidad de nombres de producto nulos son: ', null_names)\n",
    "# print()\n",
    "print('the number of duplicated name orders is: ', duplicate_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's a duplicated not null product names\n",
    "# Paso 1: Filter only by not null names\n",
    "products_with_name = df_products[df_products['product_name'].notna()]\n",
    "print(f\"the number of products with name is: {len(products_with_name)}\")\n",
    "\n",
    "# Paso 2: Change to uppercase so we can compare\n",
    "products_with_name_upper = products_with_name['product_name'].str.upper()\n",
    "\n",
    "# Paso 3: Look for duplicates\n",
    "duplicate_with_names = products_with_name_upper.duplicated().sum()\n",
    "print('the number of duplicated not null product names is: ', duplicate_with_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ba72d",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "<br>\n",
    "When analyzing the products.csv dataset, I first checked whether there were any duplicate rows. In this case, there were none. <br>\n",
    "I then verified the “product_id” column and found no duplicate values. However, in the “product_name” column, there could be hidden implicit duplicates. To address this, I converted all product names to uppercase for better comparison. <br>\n",
    "<br>\n",
    "Here are the results for the “product_id” column: <br>\n",
    "- Number of duplicate product names: 1,361\n",
    "- Number of null product names: 1,258\n",
    "- Number of products with names: 48,436\n",
    "- Number of duplicate names among non-null products: 104 <br>\n",
    "\n",
    "In conclusion, I found that some product names were missing, and after standardizing names to uppercase to correct possible implicit duplicates, only 104 duplicate product names remained among products with valid names. This allows the data exploration process to continue with cleaner information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb35c9e",
   "metadata": {},
   "source": [
    "### departments data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's duplicated rows\n",
    "print('the number of duplicated rows is: ', df_departments.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad656a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's duplicated department IDs\n",
    "print('the number of department duplicated IDs is: ', df_departments['department_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5e171",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "<br>\n",
    "By checking both the complete rows and the IDs, I confirmed that there are no duplicate data. In the case of the “department_id” column, its unique nature prevents any duplicate values from existing. Therefore, if there are no duplicates in that column, there should be no duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d08ede",
   "metadata": {},
   "source": [
    "### aisles data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdea0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's duplicated rows\n",
    "print('the number of duplicated rows is: ', df_aisles.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b11acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's duplicated aisle IDs\n",
    "print('the number of department aisle IDs is: ', df_aisles['aisle_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a98491",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "By checking both the complete rows and the IDs, I verified that there are no duplicate data. In the case of the “aisle_id” column, its unique nature prevents any duplicate values from existing. Therefore, if there are no duplicates in that column, there should be no duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e347e8",
   "metadata": {},
   "source": [
    "order_products data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b54112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if there's duplicated rows\n",
    "print('the number of duplicated rows is: ', df_order_products.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fc021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck for any other misleading duplicates\n",
    "# Here we check for duplicates by combining the columns \"add_to_cart_order\" and \"order_id\"\n",
    "duplicates_cart_orderid = df_order_products.duplicated(subset=['add_to_cart_order', 'order_id']).sum()\n",
    "print('The number of duplicates combining columns \"add_to_cart_order\" and \"order_id\" is: ', duplicates_cart_orderid)\n",
    "print()\n",
    "\n",
    "# Here we check for duplicates by combining the columns \"product_id\" and \"order_id\"\n",
    "# duplicates_cart_orderid = df_order_products.duplicated(subset=['product_id', 'order_id']).sum()\n",
    "# print('The number of duplicates combining columns \"product_id\" and \"order_id\" is: ', duplicates_cart_orderid)\n",
    "\n",
    "# Here we check for duplicates by combining the columns \"order_id\", \"product_id\", and \"reordered\"\n",
    "# duplicates_columns = df_order_products.duplicated(subset=['reordered', 'order_id', 'product_id']).sum()\n",
    "# print('The number of duplicates combining columns \"reordered\", \"order_id\", and \"product_id\" is: ', duplicates_columns)\n",
    "\n",
    "# Here we check for duplicates by combining the columns \"order_id\", \"product_id\", and \"add_to_cart_order\"\n",
    "duplicates_order_product_cart = df_order_products.duplicated(subset=['order_id', 'product_id', 'add_to_cart_order']).sum()\n",
    "print('The number of duplicates combining columns \"order_id\", \"product_id\", and \"add_to_cart_order\" is: ', duplicates_order_product_cart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796587c2",
   "metadata": {},
   "source": [
    "Findings: <br>\n",
    "We started by checking whether there were any completely duplicated rows and confirmed that none existed.\n",
    "\n",
    "To detect any misleading or inconsistent data, I began by combining the unique order number “order_id” with the sequential order “add_to_cart_order.” This seemed like a good starting point, as this combination implies that within the same unique order, each product added to the cart should have a unique sequence number.\n",
    "\n",
    "After exploring the analysis further, I concluded that it was worthwhile to include the “product_id” column in the combination, as it helps determine whether the same product appears more than once with the same sequence number within a single order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e83e5b",
   "metadata": {},
   "source": [
    "## Encuentra y elimina los valores ausentes\n",
    "\n",
    "Al trabajar con valores duplicados, pudimos observar que también nos falta investigar valores ausentes:\n",
    "\n",
    "- La columna 'product_name' de la tabla products.\n",
    "- La columna 'days_since_prior_order' de la tabla orders.\n",
    "- La columna 'add_to_cart_order' de la tabla order_productos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d5a76",
   "metadata": {},
   "source": [
    "### products data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values in the 'product_name' column\n",
    "product_name_null = df_products[df_products['product_name'].isnull()]\n",
    "print(\"Products without a name:\", len(product_name_null))\n",
    "print()\n",
    "print(product_name_null.sample(15))\n",
    "# print(product_name_null.head(15))\n",
    "# print(product_name_null.tail(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45247ef1",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "<br>\n",
    "I checked the total number of null values in the “product_name” column. <br>\n",
    "When exploring the null data, I noticed a pattern where “aisle_id” = 100 and “department_id” = 21. <br>\n",
    "I reviewed the first rows (head), the last rows (tail), and random samples (sample), and in all cases, the same pattern was repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f544a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are all missing product names related to the aisle with ID 100?\n",
    "product_name_null_aisle_100 = product_name_null[product_name_null['aisle_id'] == 100]\n",
    "all_aisle_100 = (product_name_null['aisle_id'] == 100).all()\n",
    "print(f\"Are all products without a name in aisle 100? {all_aisle_100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6709c99",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "<br>\n",
    "I filtered the “aisle_id” column for the value 100. Then, I added a line to return True/False indicating whether the products without a name belong to aisle 100. In this case, the result is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are all missing product names related to the department with ID 21?\n",
    "product_name_null_department_21 = product_name_null[product_name_null['department_id'] == 21]\n",
    "all_department_21 = (product_name_null['department_id'] == 21).all()\n",
    "print(f\"Are all products without a name in department 21? {all_department_21}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the department and aisle tables to review the data for aisle ID 100 and department ID 21.\n",
    "tables_department_aisle = df_products[(df_products['aisle_id'] == 100) & (df_products['department_id'] == 21)]\n",
    "print(tables_department_aisle.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c03f0",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "<br>\n",
    "Reviewing the list, these could correspond to non-existent aisles and/or departments, since none of the rows contain information in \"product_name.\" Alternatively, it may simply be that for that aisle and/or department, there is no \"product_name\" information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb346414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing product names with 'Unknown'\n",
    "df_products['product_name'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca36bd1",
   "metadata": {},
   "source": [
    "### orders data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb649e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find null values\n",
    "print(df_instacart_orders.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b12139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missing values that are not the customer's first order?\n",
    "order_name_null = df_instacart_orders[df_instacart_orders['days_since_prior_order'].isnull()]\n",
    "print(order_name_null.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be3f7f",
   "metadata": {},
   "source": [
    "### order_products data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75361c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find null values\n",
    "print(df_order_products.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffac14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the minimum and maximum values in this column?\n",
    "print(f\"The minimum value of the 'add_to_cart_order' column is: {df_order_products['add_to_cart_order'].min()}\")\n",
    "print(f\"The maximum value of the 'add_to_cart_order' column is: {df_order_products['add_to_cart_order'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c586d6e",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "<br>\n",
    "The isnull method was used to identify which column contains null values. In this case, it is \"add_to_cart_order\". I then calculated the maximum value (64) and the minimum value (1). In conclusion, there are no zero values — those could be interpreted as null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all order IDs that have a missing value in 'add_to_cart_order'\n",
    "cart_null_values = df_order_products[df_order_products['add_to_cart_order'].isnull()]['order_id'].unique()\n",
    "print(cart_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03dd95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do all orders with missing values have more than 64 products?\n",
    "#Group all orders with missing data by their order ID.\n",
    "#Count the number of 'product_id' in each order and check the minimum count value.\n",
    "\n",
    "filtered_order_products = df_order_products[df_order_products['order_id'].isin(cart_null_values)]\n",
    "product_counts_per_order = filtered_order_products.groupby('order_id').size()\n",
    "print(f\"The minimum value in the 'add_to_cart_order' column is: {product_counts_per_order.min()}\")\n",
    "# print(f\"The maximum value in the 'add_to_cart_order' column is: {product_counts_per_order.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c55d9c",
   "metadata": {},
   "source": [
    "Findings <br>\n",
    "<br>\n",
    "In this case, all orders with missing values do indeed have more than 64 items. Using groupby, all orders with missing data were grouped by their order_id. Finally, I calculated the min and max values to verify the range of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ccfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing values in the 'add_to_cart_order' column with 999 and convert the column to integer type.\n",
    "df_order_products['add_to_cart_order'] = df_order_products['add_to_cart_order'].fillna(999).astype(int)\n",
    "df_order_products.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11c394",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "After carrying out the data filtering process to identify missing or null values and duplicates (making sure to distinguish real duplicates from “false duplicates”), the dataset was successfully cleaned and prepared for analysis. <br>\n",
    "During the data cleaning process, specific patterns were observed — for example, in instacart_orders, duplicate data appeared in aisle 100 and department 21; and in order_products, the null values showed a pattern related to the number of products per order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d89959",
   "metadata": {},
   "source": [
    "# Step 3. Data Analysis\n",
    "Once the data is processed and ready, perform the following analysis:\n",
    "\n",
    "## [A] Initial Checks\n",
    "\n",
    "Verify that the values in the columns 'order_hour_of_day' and 'order_dow' in the orders table are reasonable (i.e., 'order_hour_of_day' ranges between 0 and 23, and 'order_dow' ranges between 0 and 6).\n",
    "- Create a chart showing the number of people placing orders depending on the time of day.\n",
    "- Create a chart showing which day of the week people make their purchases.\n",
    "- Create a chart showing how long people wait before placing their next order, and comment on the minimum and maximum values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a53e1",
   "metadata": {},
   "source": [
    "\n",
    "### [A1] Verify sensitive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f4c1a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_instacart_orders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(np.sort(\u001b[43mdf_instacart_orders\u001b[49m[\u001b[33m'\u001b[39m\u001b[33morder_hour_of_day\u001b[39m\u001b[33m'\u001b[39m].unique()))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(np.sort(df_instacart_orders[\u001b[33m'\u001b[39m\u001b[33morder_dow\u001b[39m\u001b[33m'\u001b[39m].unique()))\n",
      "\u001b[31mNameError\u001b[39m: name 'df_instacart_orders' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.sort(df_instacart_orders['order_hour_of_day'].unique()))\n",
    "print()\n",
    "print(np.sort(df_instacart_orders['order_dow'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5eea1",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "Indeed, both \"order_hour_of_day\" and \"order_dow\" are sensitive values. They fluctuate within their expected patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b8896",
   "metadata": {},
   "source": [
    "### [A2] For each hour of the day, how many people place orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9895b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_instacart_orders['order_hour_of_day'], bins=24, edgecolor='black')\n",
    "plt.title(\"Distribución de Órdenes por Hora del Día\")\n",
    "plt.xlabel(\"Hora del Día\")\n",
    "plt.ylabel(\"Número de Órdenes\")\n",
    "plt.xticks(range(24), rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007ca41",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "<br>\n",
    "To create the chart, I first had to group the data from the columns \"order_hour_of_day\" and \"order_id\" using groupby. This allowed me to obtain the total number of orders placed per hour across the 24-hour range represented in the \"order_hour_of_day\" column.\n",
    "The x-axis represents the hours, and the y-axis represents the number of orders. I used a histogram because it is visually appealing and easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceebe77",
   "metadata": {},
   "source": [
    "### [A3] On which day of the week do people buy groceries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes_por_dia = df_instacart_orders.groupby('order_dow')['order_id'].count()\n",
    "ordenes_por_dia = ordenes_por_dia.sort_index()\n",
    "\n",
    "ordenes_por_dia.plot(\n",
    "    title= \"Ordenes por Dia\",\n",
    "    kind= 'bar',\n",
    "    x= 'order_dow',\n",
    "    y= 'order_id',\n",
    "    xlabel= 'Dias de la Semana',\n",
    "    ylabel= 'Ordenes',\n",
    "    rot= 0\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d5d65",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "<br>\n",
    "To create the chart, I first had to group the data from the columns \"order_dow\" and \"order_id\" using groupby. This allowed me to obtain the total number of orders placed per day, within the 24-hour range represented in the \"order_hour_of_day\" column.\n",
    "The x-axis represents the days, and the y-axis represents the number of orders. I used a bar chart because it is visually appealing and easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc58ef5",
   "metadata": {},
   "source": [
    "### [A4] How long do people wait before placing another order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3458eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_instacart_orders['days_since_prior_order'], bins=30, edgecolor='black')\n",
    "plt.title('Distribución del tiempo de espera entre pedidos')\n",
    "plt.xlabel('Días desde el pedido anterior')\n",
    "plt.ylabel('Número de pedidos')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56357bb0",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "<br>\n",
    "We can clearly see a distinction between customers who shop very frequently—those close to 0 days since their last purchase—and those who wait up to 30 days before ordering again.\n",
    "There is a slight upward trend between days 0 and 8, after which the graph shows a steady decline up to day 30.\n",
    "This could indicate that Instacart customers tend to fall into two main groups: those who purchase very regularly and those who wait around 30 days before making another order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6daef",
   "metadata": {},
   "source": [
    "## [B] Intermediate\n",
    "\n",
    "Is there any difference between the ‘order_hour_of_day’ distributions for Wednesdays and Saturdays? Plot bar charts of ‘order_hour_of_day’ for both days in the same figure and describe any differences you observe. <br>\n",
    "Plot the distribution for the number of orders placed by customers (that is, how many customers made only 1 order, how many made 2, 3, and so on). <br>\n",
    "<br>\n",
    "Which are the top 20 most frequently ordered products (show their IDs and names)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b05a00",
   "metadata": {},
   "source": [
    "### [B1] Difference between Wednesdays and Saturdays for ‘order_hour_of_day’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cf6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "miercoles = df_instacart_orders[df_instacart_orders['order_dow'] == 3] \n",
    "miercoles_horas = miercoles['order_hour_of_day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81504cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sabado = df_instacart_orders[df_instacart_orders['order_dow'] == 6]\n",
    "sabado_horas = sabado['order_hour_of_day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(miercoles_horas.index - 0.2, miercoles_horas.values, width=0.4, label='Miércoles', color='skyblue')\n",
    "plt.bar(sabado_horas.index + 0.2, sabado_horas.values, width=0.4, label='Sábado', color='salmon')\n",
    "plt.xlabel('Hora del día')\n",
    "plt.ylabel('Número de pedidos')\n",
    "plt.title('Distribución de pedidos por hora del día (Miércoles vs Sábado)')\n",
    "plt.xticks(range(24))\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1be056",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "<br>\n",
    "For the final histogram comparing orders by hour between Wednesday and Saturday, we can observe a strong similarity in the hourly patterns. Analyzing by hour, there is a clear peak between 10 AM and 4 PM on both days. Both the high-activity periods and the low-order moments are quite similar for both days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7e9c5",
   "metadata": {},
   "source": [
    "### [B2] What is the distribution of the number of orders per customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2011a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos_por_usuario = df_instacart_orders.groupby('user_id')['order_number'].max()\n",
    "distribucion = pedidos_por_usuario.value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(distribucion.index, distribucion.values, color='skyblue')\n",
    "plt.xlabel('Número de pedidos por cliente')\n",
    "plt.ylabel('Número de clientes')\n",
    "plt.title('Distribución del número de pedidos por cliente')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396042b",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "<br>\n",
    "We can observe a high frequency from orders 1 to 4. From that point onward, the curve tends to decrease, reaching very few orders and remaining low toward the end of the chart. This could indicate that a small number of customers tend to make large purchases, while the highest peaks correspond to customers placing between 1 and 4 orders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b99d56",
   "metadata": {},
   "source": [
    "### [B3] Which are the 20 most popular products (show their ID and name)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times each product_id appears \n",
    "productos_populares = df_order_products['product_id'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the series to Dataframe\n",
    "productos_populares = productos_populares.reset_index()\n",
    "productos_populares.columns = ['product_id', 'num_pedidos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge column \"product_id\" from dataframes \"order_products\" and \"products\"\n",
    "productos_con_nombre = pd.merge(productos_populares, df_products[['product_id', 'product_name']], on='product_id')\n",
    "productos_con_nombre = productos_con_nombre[['product_id', 'product_name', 'num_pedidos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(productos_con_nombre['product_name'], productos_con_nombre['num_pedidos'], color='skyblue')\n",
    "plt.xlabel('Número de veces pedido')\n",
    "plt.ylabel('Producto')\n",
    "plt.title('Top 20 productos más pedidos')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ea163",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "<br>\n",
    "For this part, I had to merge the product_id and product_name columns from the \"products\" dataset with the \"instacart_orders\" dataset. This allowed me to create a chart showing the top 20 most ordered products. I observed that “Banana” and “Bag of Organic Bananas” stand out with over 40,000 orders each. From 20,000 orders and up, the 11 most requested products are concentrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16daf645",
   "metadata": {},
   "source": [
    "## [C] Advanced\n",
    "\n",
    "How many items do people usually buy in one order? What does the distribution look like? <br>\n",
    "What are the top 20 items that are reordered most frequently (show their product IDs and names)? <br>\n",
    "For each product, what is the reorder rate (number of reorders / total orders)?<br>\n",
    "For each customer, what proportion of the products they ordered had already been ordered before? Calculate the reorder rate for each user instead of for each product.<br>\n",
    "What are the top 20 items people add to their carts first (show the product IDs, names, and the number of times they were the first item added to the cart)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7dc469",
   "metadata": {},
   "source": [
    "### [C1] How many items do people usually buy in an order? What does the distribution look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "articulos_por_pedido = df_order_products.groupby('order_id')['product_id'].count()\n",
    "print()\n",
    "print(articulos_por_pedido.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(articulos_por_pedido, bins=range(1, 51), color='cornflowerblue', edgecolor='black')\n",
    "plt.title('Distribución de artículos por pedido')\n",
    "plt.xlabel('Número de artículos en un pedido')\n",
    "plt.ylabel('Cantidad de pedidos')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb516f",
   "metadata": {},
   "source": [
    "Conclusions <br>\n",
    "<br>\n",
    "Given the trend shown in the chart, we can see that the peak occurs within the first five items per order. From there, the curve declines exponentially, stabilizing at minimal values around 40 items. This can be interpreted as a clear distinction between customers who mostly purchase up to five items and those who add more than five to their cart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4baa24",
   "metadata": {},
   "source": [
    "### [C2] Which are the top 20 items that are most frequently reordered (show their product names and IDs)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by reordered products\n",
    "productos_populares_reordenados = df_order_products[df_order_products['reordered'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec26b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times appear each product_id\n",
    "productos_populares_reordenados = productos_populares_reordenados['product_id'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb48e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the series to Dataframe\n",
    "productos_populares_reordenados = productos_populares_reordenados.reset_index()\n",
    "productos_populares_reordenados.columns = ['product_id', 'num_reorders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fece6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the merge\n",
    "top_productos_populares_reordenados = pd.merge(productos_populares_reordenados, df_products[['product_id', 'product_name']], on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb57340",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(top_productos_populares_reordenados['product_name'], top_productos_populares_reordenados['num_reorders'], color='mediumseagreen')\n",
    "plt.title('2. Top 20 productos más reordenados')\n",
    "plt.xlabel('Número de veces reordenado')\n",
    "plt.ylabel('Producto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577efcca",
   "metadata": {},
   "source": [
    "Conclusions <br>\n",
    "<br>\n",
    "Based on the chart, these are products with high customer loyalty. We can point out specific cases that appear in the same positions as in the \"Top 20 Most Ordered Products\" chart — such as Banana, Bag of Organic Bananas, Organic Strawberries, Organic Baby Spinach, Organic Hass Avocado, and Organic Avocado — which remain customer favorites for both ordering and reordering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5483ba4",
   "metadata": {},
   "source": [
    "### [C3] For each product, what is the proportion of times it is ordered and then reordered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the repeat rate per product\n",
    "total_pedidos = df_order_products.groupby('product_id').size()\n",
    "total_reorders = df_order_products.groupby('product_id')['reordered'].sum()\n",
    "tasa_reorden_producto = (total_reorders / total_pedidos).reset_index(name='tasa_reorden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44517ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(tasa_reorden_producto['tasa_reorden'], bins=50, color='coral', edgecolor='black')\n",
    "plt.title('3. Tasa de repetición por producto')\n",
    "plt.xlabel('Tasa de repetición')\n",
    "plt.ylabel('Cantidad de productos')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd755e4",
   "metadata": {},
   "source": [
    "Conclusions <br>\n",
    "<br>\n",
    "In this case, the reorder rate was calculated per product. It can be said that although many products are ordered only once, there is a small number of items with a very high reorder rate. There is a clear trend among customers not to repeat their purchases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a49b8e",
   "metadata": {},
   "source": [
    "### [C4] For each customer, what proportion of their ordered products had they already ordered before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge order_products and orders to get user_id\n",
    "df_merged = pd.merge(df_order_products, df_instacart_orders[['order_id', 'user_id']], on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by user\n",
    "user_total = df_merged.groupby('user_id')['product_id'].count()\n",
    "user_reordered = df_merged.groupby('user_id')['reordered'].sum()\n",
    "tasa_reorden_usuario = (user_reordered / user_total).reset_index(name='tasa_reorden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3907ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(tasa_reorden_usuario['tasa_reorden'], bins=50, color='slateblue', edgecolor='black')\n",
    "plt.title('4. Tasa de repetición por usuario')\n",
    "plt.xlabel('Tasa de repetición')\n",
    "plt.ylabel('Cantidad de usuarios')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb73d43",
   "metadata": {},
   "source": [
    "Conclusions <br>\n",
    "<br>\n",
    "Analyzing this reorder rate among customers, we can conclude that many customers tend to try new products with each order. However, there is a niche group of loyal customers who consistently repurchase certain products. It may be worthwhile to analyze these customers more deeply as part of a retention or personalization strategy aimed at them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db172e",
   "metadata": {},
   "source": [
    "### [C5] What are the top 20 items that people add to their carts first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53393504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by add_to_cart_order == 1\n",
    "primeros_productos = df_order_products[df_order_products['add_to_cart_order'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each product was first ordered\n",
    "top_primero = (primeros_productos['product_id'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d11552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the series to Dataframe\n",
    "top_primero = top_primero.reset_index()\n",
    "top_primero.columns = ['product_id', 'veces_primero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the name of product\n",
    "top_primero = pd.merge(top_primero, df_products[['product_id', 'product_name']], on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496455e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(top_primero['product_name'], top_primero['veces_primero'], color='goldenrod')\n",
    "plt.title('5. Top 20 productos añadidos primero al carrito')\n",
    "plt.xlabel('Veces como primer producto')\n",
    "plt.ylabel('Producto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b035739",
   "metadata": {},
   "source": [
    "# Conclusion: \n",
    "\n",
    "This project provided a deep exploration into consumer behavior patterns on the Instacart platform. By analyzing the five major datasets: orders, products, order_products, aisles, and departments. We successfully navigated real-world data to extract valuable and actionable insights.\n",
    "\n",
    "Preprocessing Stage: Foundation of the Analysis <br>\n",
    "<br>\n",
    "The data cleaning phase was essential and revealed significant challenges. The first crucial step was identifying the correct data separator (semicolon instead of comma). Handling duplicate values was insightful: we found 15 exact duplicates in the orders table, all anomalously grouped on Wednesdays at 2 a.m., which were subsequently removed.\n",
    "\n",
    "- Product Names: The 1,258 unnamed products (product_name) were not random; all belonged to aisle 100 and department 21 (identified as “missing” or “unknown”). These were correctly imputed as “Unknown” to maintain record integrity.\n",
    "- Days Since Prior Order: The 28,817 missing values in days_since_prior_order perfectly corresponded to orders where order_number was 1 — confirming that these were new customers placing their first order, a meaningful insight on its own.\n",
    "- Cart Order: The 836 missing values in add_to_cart_order were associated with exceptionally large orders (over 64 items, the observed maximum). These were imputed with an outlier value (999) for proper handling.\n",
    "\n",
    "Exploratory Data Analysis: Discovering Key Patterns <br>\n",
    "Once cleaned, the analysis revealed clear behavioral trends about when and how customers shop: <br>\n",
    "\n",
    "- Peak Hours: Ordering activity is strongly concentrated between 10:00 a.m. and 4:00 p.m. This trend held steady across both high-demand days (like Saturday) and lower-demand days (like Wednesday).\n",
    "- Purchase Frequency: A strong bimodal pattern emerged (A4). Two main customer groups were identified: a weekly segment (peaking around 7 days) and a monthly segment (peaking at 30 days).\n",
    "- Order Size & Customer Loyalty: Most customers place only a few orders (1–4) (B2) and buy a small number of items per order (the peak is around 5 items) (C1). However, there is a “long tail” of highly recurrent customers who make much larger purchases.\n",
    "- Reorder Behavior: The reorder analysis (C3, C4) revealed polarization — most products and users exhibit low reorder rates (favoring exploration), but there exists a loyal user segment that reorders almost all of their products in every purchase.\n",
    "\n",
    "Global Insight: The “Anchor Product” and Customer Segmentation <br>\n",
    "<br>\n",
    "The most consistent finding across the entire analysis was the dominance of specific products. “Banana” and “Bag of Organic Bananas” were not only the most purchased items (B3) but also the most reordered (C2) and, crucially, the most frequently added first to the cart (C5).\n",
    "This suggests that these products — along with other fresh organic items like strawberries and avocados — act as “anchor products”: primary intention items that initiate the shopping process.\n",
    "\n",
    "In Summary: <br>\n",
    "<br>\n",
    "Analyzing these five datasets allowed us to identify distinct customer archetypes — weekly vs. monthly, explorers vs. loyalists, small vs. large basket shoppers. For Instacart, this translates into clear strategic opportunities:\n",
    "\n",
    "- Interface Optimization: Highlighting anchor products (like bananas) at the start of a user session could reduce friction and increase cart conversion.\n",
    "- Personalized Marketing: Users can be segmented — explorers (low reorder rate) can receive discounts on new products, while loyal customers (high reorder rate) could benefit from “quick reorder” or subscription options for their favorite items."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
